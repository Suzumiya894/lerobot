"""
è¯¥è„šæœ¬ç”¨äºåå¤„ç†STRETCHé‡‡é›†çš„æ•°æ®é›†ï¼ŒåŸå§‹æ•°æ®é›†åŒ…æ‹¬ä¸‰ä¸ªç›¸æœºnavigation, head, wristï¼ŒåŒ…å«11ä¸ªè‡ªç”±åº¦ï¼Œobservation.stateä¸ºå…³èŠ‚çš„ç»å¯¹ä½ç½®ï¼Œactionä¸ºå…³èŠ‚çš„é€Ÿåº¦ã€‚
ä¸€èˆ¬è®­ç»ƒæ—¶ä½¿ç”¨åªä½¿ç”¨headå’Œwristçš„å›¾åƒæ•°æ®ï¼Œå¹¶ä¸”ä¹Ÿç”¨ä¸åˆ°å…¨éƒ¨11ç»´è‡ªç”±åº¦ï¼Œå› æ­¤éœ€è¦åå¤„ç†ã€‚ï¼ˆnavigationå›¾åƒæ•°æ®éœ€è¦æ‰‹åŠ¨åˆ é™¤ï¼‰
è¯¥è„šæœ¬å¯ä»¥åˆ é™¤æŒ‡å®šçš„åˆ—ï¼ˆä¾‹å¦‚head_pan.pos, head_tilt.posç­‰ï¼‰ï¼Œå¹¶ä¸”æä¾›ä¸¤ç§å¤„ç†actionçš„æ–¹å¼ï¼š
1. å°†actionä»é€Ÿåº¦ï¼ˆvelï¼‰è½¬æ¢ä¸ºä½ç½®ï¼ˆposï¼‰ï¼Œå³å°†ä¸‹ä¸€å¸§çš„observation.stateä½œä¸ºå½“å‰å¸§çš„actionï¼šprocess_dataset_vel_to_pos()
2. ä¿ç•™actionçš„é€Ÿåº¦ä¿¡æ¯ï¼ˆvelï¼‰ï¼šprocess_dataset_keep_vel()
"""
import os
import json
import pandas as pd

FEATURES = ['head_pan.pos', 'head_tilt.pos', 'lift.pos', 'arm.pos',
            'wrist_pitch.pos', 'wrist_roll.pos', 'wrist_yaw.pos', 
            'gripper.pos', 'base_x.pos', 'base_y.pos', 'base_theta.pos']
FEATURE_IDX_NAMES = {name: idx for idx, name in enumerate(FEATURES)}
def process_dataset_vel_to_pos(dataset_path: str, to_pop_names:list[str] = ["head_pan.pos", "head_tilt.pos"]) -> None:
    """
    ä¸€ä¸ªç”¨äºä¿®æ”¹Lerobotæ•°æ®é›†çš„å‡½æ•°ã€‚è¯¥å‡½æ•°ä¼šåˆ é™¤to_pop_namesä¸­æŒ‡å®šçš„åˆ—åï¼Œå¹¶ä¸”å°†åŸæœ¬æ•°æ®æ—¶velçš„actionæ”¹ä¸ºposçš„actionã€‚

    è¯¥å‡½æ•°ä¼šæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
    2. å¤„ç†episode_stats.jsonlæ–‡ä»¶ï¼Œåˆ é™¤æŒ‡å®šçš„åˆ—åï¼Œå¹¶ä¿®æ”¹actionçš„ç»Ÿè®¡æ•°æ®ã€‚
    3. è¿­ä»£å¤„ç†data/chunk-000/ä¸­çš„æ‰€æœ‰Parquetæ–‡ä»¶ã€‚

    Args:
        dataset_path (str): Lerobotæ•°æ®é›†çš„æ ¹ç›®å½•è·¯å¾„ã€‚
    """
    print(f"æ­£åœ¨å¤„ç†æ•°æ®é›†: {os.path.abspath(dataset_path)}\n")

    
    to_pop_idxs = [FEATURE_IDX_NAMES[name] for name in to_pop_names]
    new_len = len(FEATURES) - len(to_pop_names) 

    # --- 1. åˆ é™¤ `observation.images.navigation` ç›®å½• ---
    # æ‰‹åŠ¨åˆ é™¤å³å¯

    # --- 2. è¯»å–å¹¶æ˜¾ç¤º meta æ–‡ä»¶å¤¹ä¸­å…ƒæ•°æ®çš„ç»“æ„ ---
    meta_path = os.path.join(dataset_path, 'meta')
    print("\n--- æ­£åœ¨è¯»å–å…ƒæ•°æ® (meta) ---")

    # é¦–å…ˆå¤„ç† info.json
    try:
        info_json_path = os.path.join(meta_path, 'info.json')
        with open(info_json_path, 'r', encoding='utf-8') as f:
            info_data = json.load(f)
            print("\nğŸ“„ `info.json` çš„ç»“æ„:")
            print(info_data)
            for key, value in info_data.items():
                print(f"  {key}: {value}")
                print("-" * 40)

            info_data['total_frames'] # ï¼Ÿè¿™ä¸ªéœ€è¦ä¿®æ”¹å—
            info_data['total_videos'] = info_data['total_episodes'] * 2 # ä¸¤ä¸ªæ‘„åƒå¤´ï¼Œæ•°é‡ä¹˜2
            info_data['features']['action']['shape'] = [new_len]
            info_data['features']['observation.state']['shape'] = [new_len]
            # åŸæœ¬çš„'names': ['head_pan.vel', 'head_tilt.vel', 'lift.vel', 'arm.vel', 'wrist_pitch.vel', 'wrist_roll.vel', 'wrist_yaw.vel', 'gripper.vel', 'base_x.vel', 'base_y.vel', 'base_theta.vel']

            kept_features = [name for name in FEATURE_IDX_NAMES if name not in to_pop_names]
            info_data['features']['observation.state']['names'] = kept_features
            info_data['features']['action']['names'] = [f"{name.replace('.pos', '.next_pos')}" for name in kept_features]

            # info_data['features']['action']['names'] = ['lift.next_pos', 'arm.next_pos', 'wrist_pitch.next_pos', 'wrist_roll.next_pos', 'wrist_yaw.next_pos', 'gripper.next_pos', 'base_x.next_pos', 'base_y.next_pos', 'base_theta.next_pos']
            # info_data['features']['observation.state']['names'] = ['lift.pos', 'arm.pos', 'wrist_pitch.pos', 'wrist_roll.pos', 'wrist_yaw.pos', 'gripper.pos', 'base_x.pos', 'base_y.pos', 'base_theta.pos']

            info_data['features'].pop("observation.images.navigation", None)
        
        with open(info_json_path, 'w', encoding='utf-8') as f:
            json.dump(info_data, f, indent=4)
    
    except FileNotFoundError:
        print(f"âš ï¸ æœªæ‰¾åˆ° `info.json`ã€‚")


    # æ¥ç€å¤„ç† .jsonl æ–‡ä»¶ (episodes.jsonl, episodes_stats.jsonl, tasks.jsonl)
    # for filename in ['episodes.jsonl', 'episodes_stats.jsonl', 'tasks.jsonl']:
    filename = 'episodes_stats.jsonl'   # åªéœ€è¦å¤„ç†episodes_stats.jsonl
    modified_episodes_stats = []
    jsonl_path = os.path.join(meta_path, filename)
    try:
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            first_line = True
            for line in f:
                data = json.loads(line)
                if first_line:
                    print(f"\nğŸ“„ `{filename}` çš„ç¬¬ä¸€è¡Œæ•°æ®:")
                    print(data)
                    first_line = False

                for key, value in data['stats']['observation.state'].items():
                    if key == 'count':
                        continue
                    if len(value) != len(FEATURES):
                        # é¿å…é‡å¤æ‰§è¡Œè¯¥è„šæœ¬å‡ºç°é”™è¯¯
                        continue

                    data['stats']['observation.state'][key] = [value[i] for i in range(len(value)) if i not in to_pop_idxs]
                    # data['stats']['observation.state'][key] = value[2:] # åˆ é™¤å‰ä¸¤ä¸ªå…ƒç´ 
                
                data['stats']['action'] = data['stats']['observation.state']  # ä¿®æ”¹ action çš„ç»Ÿè®¡æ•°æ®
                data['stats'].pop("observation.images.navigation", None)  # åˆ é™¤ navigation å›¾åƒçš„ç»Ÿè®¡æ•°æ®

                modified_episodes_stats.append(data)
            
        # å°†ä¿®æ”¹åçš„æ•°æ®å†™å›åˆ°åŒä¸€ä¸ªæ–‡ä»¶
        with open(jsonl_path, 'w', encoding='utf-8') as f:
            for item in modified_episodes_stats:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
                
    except FileNotFoundError:
        print(f"âš ï¸ æœªæ‰¾åˆ° `{filename}`ã€‚")

    # --- 3. è¿­ä»£å¤„ç† data/chunk-000/ ä¸­çš„æ‰€æœ‰ Parquet æ–‡ä»¶ ---
    data_chunk_path = os.path.join(dataset_path, 'data', 'chunk-000')
    print(f"\n--- æ­£åœ¨è¿­ä»£å¤„ç† Parquet æ–‡ä»¶äº: {data_chunk_path} ---")

    if not os.path.exists(data_chunk_path):
        print(f"âŒ é”™è¯¯: æ•°æ®ç›®å½•ä¸å­˜åœ¨ {data_chunk_path}")
        return

    # è·å–æ‰€æœ‰.parquetæ–‡ä»¶çš„åˆ—è¡¨
    parquet_files = [f for f in os.listdir(data_chunk_path) if f.endswith('.parquet')]
    parquet_files.sort()

    for i, filename in enumerate(parquet_files):
        file_path = os.path.join(data_chunk_path, filename)
        print(f"  ({i+1}/{len(parquet_files)}) æ­£åœ¨å¤„ç†: {filename}")

        # å°† Parquet æ–‡ä»¶è¯»å…¥ pandas DataFrame
        df = pd.read_parquet(file_path)

        # ===============================================================
        # 1. ä¿®æ”¹ 'observation.state' åˆ—
        #    å¯¹è¯¥åˆ—çš„æ¯ä¸€è¡Œåº”ç”¨ä¸€ä¸ªlambdaå‡½æ•°ï¼Œè£æ‰åˆ—è¡¨çš„å‰ä¸¤ä¸ªå…ƒç´ 
        #    æˆ‘ä»¬å…ˆå°†ç»“æœå­˜å‚¨åœ¨ä¸€ä¸ªæ–°çš„Seriesä¸­ï¼Œå› ä¸ºå®ƒåœ¨ä¸‹ä¸€æ­¥ä¸­éœ€è¦è¢«ä½¿ç”¨
        if len(df['observation.state'][0]) != len(FEATURES):
            print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {filename}ï¼Œå› ä¸º 'observation.state' åˆ—çš„é•¿åº¦ä¸æ˜¯ {len(FEATURES)}ã€‚")
            continue
        modified_state = df['observation.state'].apply(lambda state_list: [state_list[i] for i in range(len(state_list)) if i not in to_pop_idxs])
        # modified_state = df['observation.state'].apply(lambda state_list: state_list[2:])

        # 2. ä¿®æ”¹ 'action' åˆ—
        #    æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç”¨ pandas çš„ shift(-1) æ–¹æ³•ï¼Œå®ƒèƒ½å°†åˆ—çš„æ•°æ®å‘ä¸Šç§»åŠ¨ä¸€è¡Œ
        #    è¿™æ ·ç¬¬ idx è¡Œå°±èƒ½å–åˆ°åŸå§‹çš„ç¬¬ idx+1 è¡Œçš„æ•°æ®
        new_actions = modified_state.shift(-1)

        #    å¤„ç†æœ€åä¸€è¡Œï¼šshift(-1) ä¼šåœ¨æœ€åä¸€è¡Œäº§ç”Ÿä¸€ä¸ªç©ºå€¼ (NaN)
        #    æ ¹æ®ä½ çš„è§„åˆ™ï¼Œæœ€åä¸€è¡Œactionåº”è¢«èµ‹äºˆåŒè¡Œçš„observation.state
        #    æˆ‘ä»¬ç”¨ modified_state çš„æœ€åä¸€è¡Œå€¼æ¥å¡«å……è¿™ä¸ªç©ºå€¼
        last_row_index = df.index[-1]
        last_state = modified_state.loc[last_row_index]
        
        # å½“Seriesä¸­åŒ…å«åˆ—è¡¨ç­‰å¯¹è±¡æ—¶ï¼Œç›´æ¥ç”¨fillnaå¯èƒ½è¡Œä¸ºä¸ä¸€è‡´
        # æ›´ç¨³å¦¥çš„æ–¹å¼æ˜¯ç›´æ¥å®šä½å¹¶èµ‹å€¼
        # å°†new_actionsè½¬æ¢ä¸ºobjectç±»å‹ä»¥æ¥å—åˆ—è¡¨èµ‹å€¼
        new_actions = new_actions.astype(object)
        new_actions.loc[last_row_index] = last_state

        # 3. å°†ä¿®æ”¹åçš„åˆ—èµ‹å€¼å› DataFrame
        df['observation.state'] = modified_state
        df['action'] = new_actions
        
        print(f"    -> åˆ— 'observation.state' å’Œ 'action' å·²æˆåŠŸä¿®æ”¹ã€‚")
        # ===============================================================

        # å°†ä¿®æ”¹åçš„ DataFrame å†™å›åŸæ¥çš„ Parquet æ–‡ä»¶ï¼Œè¦†ç›–æ—§æ–‡ä»¶
        df.to_parquet(file_path)
        

    print("\nâœ… æ‰€æœ‰ Parquet æ–‡ä»¶å¤„ç†å®Œæ¯•ï¼")


def process_dataset_keep_vel(dataset_path: str, to_pop_names:list[str] = ["head_pan.pos", "head_tilt.pos"]):
    """
    actionä¿ç•™ä¸ºé€Ÿåº¦ä¿¡æ¯ï¼Œåªå°†head_panï¼Œ head_tiltåˆ é™¤
    """
    print(f"æ­£åœ¨å¤„ç†æ•°æ®é›†: {os.path.abspath(dataset_path)}\n")


    to_pop_idxs = [FEATURE_IDX_NAMES[name] for name in to_pop_names]
    new_len = len(FEATURES) - len(to_pop_names) 

    # --- 1. åˆ é™¤ `observation.images.navigation` ç›®å½• ---
    # æ‰‹åŠ¨åˆ é™¤å³å¯

    # --- 2. è¯»å–å¹¶æ˜¾ç¤º meta æ–‡ä»¶å¤¹ä¸­å…ƒæ•°æ®çš„ç»“æ„ ---
    meta_path = os.path.join(dataset_path, 'meta')
    print("\n--- æ­£åœ¨è¯»å–å…ƒæ•°æ® (meta) ---")

    # é¦–å…ˆå¤„ç† info.json
    try:
        info_json_path = os.path.join(meta_path, 'info.json')
        with open(info_json_path, 'r', encoding='utf-8') as f:
            info_data = json.load(f)
            print("\nğŸ“„ `info.json` çš„ç»“æ„:")
            print(info_data)
            for key, value in info_data.items():
                print(f"  {key}: {value}")
                print("-" * 40)

            info_data['total_frames'] # ï¼Ÿè¿™ä¸ªéœ€è¦ä¿®æ”¹å—
            info_data['total_videos'] = info_data['total_episodes'] * 2 # ä¸¤ä¸ªæ‘„åƒå¤´ï¼Œæ•°é‡ä¹˜2
            info_data['features']['action']['shape'] = [new_len]
            info_data['features']['observation.state']['shape'] = [new_len]
            # åŸæœ¬çš„'names': ['head_pan.vel', 'head_tilt.vel', 'lift.vel', 'arm.vel', 'wrist_pitch.vel', 'wrist_roll.vel', 'wrist_yaw.vel', 'gripper.vel', 'base_x.vel', 'base_y.vel', 'base_theta.vel']

            kept_features = [name for name in FEATURE_IDX_NAMES if name not in to_pop_names]
            info_data['features']['observation.state']['names'] = kept_features
            info_data['features']['action']['names'] = [f"{name.replace('.pos', '.vel')}" for name in kept_features]

            # info_data['features']['action']['names'] = ['lift.next_pos', 'arm.next_pos', 'wrist_pitch.next_pos', 'wrist_roll.next_pos', 'wrist_yaw.next_pos', 'gripper.next_pos', 'base_x.next_pos', 'base_y.next_pos', 'base_theta.next_pos']
            # info_data['features']['observation.state']['names'] = ['lift.pos', 'arm.pos', 'wrist_pitch.pos', 'wrist_roll.pos', 'wrist_yaw.pos', 'gripper.pos', 'base_x.pos', 'base_y.pos', 'base_theta.pos']

            info_data['features'].pop("observation.images.navigation", None)
        
        with open(info_json_path, 'w', encoding='utf-8') as f:
            json.dump(info_data, f, indent=4)
    
    except FileNotFoundError:
        print(f"âš ï¸ æœªæ‰¾åˆ° `info.json`ã€‚")


    # æ¥ç€å¤„ç† .jsonl æ–‡ä»¶ (episodes.jsonl, episodes_stats.jsonl, tasks.jsonl)
    # for filename in ['episodes.jsonl', 'episodes_stats.jsonl', 'tasks.jsonl']:
    filename = 'episodes_stats.jsonl'   # åªéœ€è¦å¤„ç†episodes_stats.jsonl
    modified_episodes_stats = []
    jsonl_path = os.path.join(meta_path, filename)
    try:
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            first_line = True
            for line in f:
                data = json.loads(line)
                if first_line:
                    print(f"\nğŸ“„ `{filename}` çš„ç¬¬ä¸€è¡Œæ•°æ®:")
                    print(data)
                    first_line = False

                for tp in ['observation.state', 'action']:
                    for key, value in data['stats'][tp].items():
                        if key == 'count':
                            continue
                        if len(value) != len(FEATURES):
                            # é¿å…é‡å¤æ‰§è¡Œè¯¥è„šæœ¬å‡ºç°é”™è¯¯
                            continue

                        data['stats'][tp][key] = [value[i] for i in range(len(value)) if i not in to_pop_idxs]
                
                data['stats'].pop("observation.images.navigation", None)  # åˆ é™¤ navigation å›¾åƒçš„ç»Ÿè®¡æ•°æ®

                modified_episodes_stats.append(data)
            
        # å°†ä¿®æ”¹åçš„æ•°æ®å†™å›åˆ°åŒä¸€ä¸ªæ–‡ä»¶
        with open(jsonl_path, 'w', encoding='utf-8') as f:
            for item in modified_episodes_stats:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
                
    except FileNotFoundError:
        print(f"âš ï¸ æœªæ‰¾åˆ° `{filename}`ã€‚")

    # --- 3. è¿­ä»£å¤„ç† data/chunk-000/ ä¸­çš„æ‰€æœ‰ Parquet æ–‡ä»¶ ---
    data_chunk_path = os.path.join(dataset_path, 'data', 'chunk-000')
    print(f"\n--- æ­£åœ¨è¿­ä»£å¤„ç† Parquet æ–‡ä»¶äº: {data_chunk_path} ---")

    if not os.path.exists(data_chunk_path):
        print(f"âŒ é”™è¯¯: æ•°æ®ç›®å½•ä¸å­˜åœ¨ {data_chunk_path}")
        return

    # è·å–æ‰€æœ‰.parquetæ–‡ä»¶çš„åˆ—è¡¨
    parquet_files = [f for f in os.listdir(data_chunk_path) if f.endswith('.parquet')]
    parquet_files.sort()
    for i, filename in enumerate(parquet_files):
        file_path = os.path.join(data_chunk_path, filename)
        print(f"  ({i+1}/{len(parquet_files)}) æ­£åœ¨å¤„ç†: {filename}")

        # å°† Parquet æ–‡ä»¶è¯»å…¥ pandas DataFrame
        df = pd.read_parquet(file_path)

        # ===============================================================
        # 1. ä¿®æ”¹ 'observation.state' åˆ—
        #    å¯¹è¯¥åˆ—çš„æ¯ä¸€è¡Œåº”ç”¨ä¸€ä¸ªlambdaå‡½æ•°ï¼Œè£æ‰åˆ—è¡¨çš„å‰ä¸¤ä¸ªå…ƒç´ 
        #    æˆ‘ä»¬å…ˆå°†ç»“æœå­˜å‚¨åœ¨ä¸€ä¸ªæ–°çš„Seriesä¸­ï¼Œå› ä¸ºå®ƒåœ¨ä¸‹ä¸€æ­¥ä¸­éœ€è¦è¢«ä½¿ç”¨
        if len(df['observation.state'][0]) != len(FEATURES):
            print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {filename}ï¼Œå› ä¸º 'observation.state' åˆ—çš„é•¿åº¦ä¸æ˜¯ 11ã€‚")
            continue
        modified_state = df['observation.state'].apply(lambda state_list: [state_list[i] for i in range(len(state_list)) if i not in to_pop_idxs])
        modified_action = df['action'].apply(lambda action_list: [action_list[i] for i in range(len(action_list)) if i not in to_pop_idxs])

        df['observation.state'] = modified_state
        df['action'] = modified_action
        
        print(f"    -> åˆ— 'observation.state' å’Œ 'action' å·²æˆåŠŸä¿®æ”¹ã€‚")
        # ===============================================================

        # å°†ä¿®æ”¹åçš„ DataFrame å†™å›åŸæ¥çš„ Parquet æ–‡ä»¶ï¼Œè¦†ç›–æ—§æ–‡ä»¶
        df.to_parquet(file_path)
        

    print("\nâœ… æ‰€æœ‰ Parquet æ–‡ä»¶å¤„ç†å®Œæ¯•ï¼")
    


if __name__ == "__main__":
    YOUR_DATASET_ROOT = "/data/yew/huggingface/lerobot/Suzumiya894/smolvla_dataset2_vel/"

    to_pop_features = ["head_pan.pos", "head_tilt.pos", "base_y.pos", "base_theta.pos"]  # éœ€è¦åˆ é™¤çš„ç‰¹å¾å
    KEEP_VEL = True  # æ˜¯å¦å°†é€Ÿåº¦ä½œä¸ºACTION
    if KEEP_VEL:
        process_dataset_keep_vel(YOUR_DATASET_ROOT, to_pop_names=to_pop_features)
    else:
        # å°†next_posä½œä¸ºACTION
        process_dataset_vel_to_pos(YOUR_DATASET_ROOT, to_pop_names=to_pop_features)
